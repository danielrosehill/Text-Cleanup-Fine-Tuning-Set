# Text Cleanup Fine-Tuning Dataset

[![Hugging Face Dataset](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-Dataset-yellow)](https://huggingface.co/datasets/danielrosehill/Transcription-Cleanup-Trainer)

A dataset in progress for fine-tuning models to clean up speech-to-text transcripts.

## What This Is

A collection of speech transcripts with multiple versions showing different levels of cleanup, intended for fine-tuning models to achieve optimal transcript cleanup ("Goldilocks" level - not too much, not too little).

## Fine-Tuning Objective

The goal is to generate a fine-tuned audio multimodal model (Hugging Face task: audio-text to text) that can automatically produce cleaned transcripts matching the quality and style demonstrated in the manual cleanup examples.

**Dataset Components:**
- **Audio file**: Source recording (2-5 minutes)
- **Whisper ASR transcript**: Verbatim baseline from speech-to-text (includes all filler words and disfluencies)
- **Manual cleanup transcript**: Ground truth target demonstrating the desired cleanup quality

The fine-tuned model should learn to transform audio directly into text matching the manual cleanup style - removing filler words and disfluencies while preserving natural tone and meaning.

**Note:** Sample 1 includes an auto-cleanup transcript from Gemini 2.5 Flash as a reference example, showing the deviation between a general-purpose model and the target cleanup quality. This helps illustrate why fine-tuning is necessary. Future samples will only include audio, Whisper transcript, and manual cleanup.

## Repository Structure

```
Text-Cleanup-Fine-Tuning-Set/
â”‚
â”œâ”€â”€ dataset/                      # The actual dataset
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ audio/               # Original audio recordings
â”‚   â”‚   â”œâ”€â”€ whisper-transcripts/ # Raw Whisper ASR output
â”‚   â”‚   â”œâ”€â”€ auto-cleanup/        # Sample 1 reference (Gemini)
â”‚   â”‚   â””â”€â”€ manual-cleanups/     # Human-edited ground truth
â”‚   â”œâ”€â”€ questions.json           # Question metadata
â”‚   â”œâ”€â”€ dataset.json             # Complete dataset metadata
â”‚   â””â”€â”€ README.md               # Dataset-specific documentation
â”‚
â”œâ”€â”€ ai-analysis/                 # Fine-tuning planning & strategy
â”‚   â”œâ”€â”€ sample-size-recommendations.md  # Target sample counts
â”‚   â”œâ”€â”€ dataset-structure-mapping.md    # Component relationships
â”‚   â””â”€â”€ model-candidates.md             # Audio-text models to fine-tune
â”‚
â”œâ”€â”€ divergence-analysis/         # Sample-level cleanup analysis
â”‚   â”œâ”€â”€ sample-1-analysis.md    # Divergence patterns in Sample 1
â”‚   â””â”€â”€ README.md               # Analysis methodology
â”‚
â””â”€â”€ Tools for dataset creation
    â”œâ”€â”€ transcript_recorder.py   # GUI for recording and processing
    â”œâ”€â”€ dataset_builder.py       # Dataset management and export
    â”œâ”€â”€ process_audio.py         # CLI audio processor
    â””â”€â”€ requirements.txt         # Python dependencies
```

## The Pipeline

Each sample goes through this process:

1. **Audio Recording** - Voice response to a question (2-5 minutes)
2. **Whisper Transcription** - Raw ASR output with filler words and disfluencies
3. **Manual Cleanup** - Human-edited ground truth showing target quality

The manual cleanup is the training target, demonstrating the desired level of cleanup.

## Dataset Format

The `dataset.json` file contains comprehensive metadata for each sample:

- Sample ID and question
- File paths to all versions
- Audio metadata (duration, format)
- Text statistics (word counts)
- Models used
- Processing status

Export formats available: JSON, JSONL (for training)

## Usage

### Recording New Samples

```bash
source .venv/bin/activate
python transcript_recorder.py
```

GUI walks through: select question â†’ record audio â†’ automatic processing â†’ create manual cleanup

### Managing the Dataset

```bash
# Build/update metadata
python dataset_builder.py build

# Check completeness
python dataset_builder.py validate

# Export for training
python dataset_builder.py export jsonl
```

## Cleanup Philosophy

**Remove:**
- Filler words (um, uh, like)
- False starts and repetitions
- Disfluencies

**Preserve:**
- Natural conversational tone
- Speaker's meaning
- Personality and voice

## Current Status

See `dataset/dataset.json` for current sample count and completion status.

## Setup

```bash
# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Configure API keys in .env
OPENAI_API_KEY=your_key_here
OPENROTUER_API_KEY=your_key_here
TEXT_CLEANUP_VALIDATION_MODEL=google/gemini-2.5-flash
```

## License

MIT License

## Author

Daniel Rosehill
- Website: danielrosehill.com
- Email: public@danielrosehill.com
